# 컴퓨터가 이해하는 정보

컴퓨터는 기본적으로 0과 1만 이해할 수 있습니다.

이러한 0과 1을 나타내는 가장 작은 정보의 단위가 **비트**인데 1비트는 2개의 정보를, N개의 비트는 2^N개의 정보를 표현할 수 있습니다.

하지만 비트는 너무나도 작은 단위이기 때문에 우리는 보통 프로그램의 크기를 말할 때 비트보다 큰 단위인 **바이트**를 기본 단위로 사용하는데 1byte = 8bit 입니다.

바이트를 사용한 킬로바이트(kB), 메가바이트(MB), 기가바이트(GB), 테라바이트(TB)는 모두 이전 단위 1,000개를 묶은 단위를 말합니다. 

<img src="https://github.com/user-attachments/assets/8435cf0f-8f4a-49dc-af14-633b53ff0d2d" width="250" height="200">

그런데 이런 byte, kB, MB, GB, TB는 프로그램의 크기를 나타낼 때 사용하는 단위입니다.

즉 **프로그램**의 관점에서 본 정보 단위인 것인데 반면에 **CPU** 관점에서 본 정보 단위는 **워드**라고 합니다.

워드(word)란, CPU가 1번에 처리할 수 있는 데이터의 크기를 의미합니다. 따라서 CPU는 워드 단위로 프로그램을 읽고 처리합니다.

워드의 크기는 CPU마다 다르지만 보통 1word = 32bit or 64bit 입니다.

## 데이터 - 0과 1로 숫자 표현하기

CPU는 컴퓨터 내부에서 2진법을 사용해 데이터를 이해합니다. 그러나 2진법은 표현하는 숫자의 길이가 너무 길어진다는 단점이 있습니다. 

10진수인 '128'을 2진법으로 표현하더라도 8자리의 숫자가 필요합니다.

따라서 2진수와 더불어 16진수를 사용하기도 합니다. 16진수로 표현된 수는 아래첨자로 (16)을 붙이거나 16진수 앞에 0x를 붙입니다. 

예시로 16진수는 MAC 주소나 IPv6 주소를 표현할 때 사용됩니다. 

우리가 이미 알고 있듯이 정수를 2진수로 표현하는 방법은 어렵지 않습니다. 그렇다면 2진수로 정수가 아닌 **소수**는 어떻게 표현할 수 있을까요? 

**이 때 우리가 기억해야 할 점은 컴퓨터의 소수 표현에는 오차가 존재할 수 있다는 점입니다.**

```python
a = 0.1
b = 0.2
c = 0.3

if a + b == c:
    print("Equal")
else:
    print("Not Equal")
```

위와 같은 파이썬 코드를 실행해보면 "Equal"을 출력할 것 같지만 "Not Equal"을 출력합니다. 

파이썬은 물론이고 C/C++, Java, JavaScript 등 많은 언어에서 같은 결과를 보입니다.

이러한 오차가 발생하는 이유가 무엇일까요? 

컴퓨터는 소수를 나타내기 위해 **부동 소수점(Floating Point)** 표현 방식을 사용하는데 이 방식은 정밀도의 한계가 있기 때문입니다.

부동 소수점은 소수점이 고정되어 있지 않은 소수 표현 방식으로, 필요에 따라 소수점의 위치가 이동할 수 있어 부동 소수점이라는 이름이 붙었습니다.

예를 들어 123.123을 부동소수점 방식으로 표현한다면, 1.23123 x 10^2, 12.3123 x 10^1 등 소수점의 위치를 바꾸더라도 같은 숫자를 표현할 수 있습니다. 

여기서 제곱으로 표현된 2와 -1을 **지수(Exponential)**, 1.23123과 12.3123을 **가수(Significant)** 라고 합니다.

2진수 체계에서도 이와 유사하게 소수를 m x 2^n 꼴로 표현합니다 (m : 가수, n : 지수). 

예를 들어 10진수 소수 107.6640625를 2진수로 나타내면 11010110.10101 x 2^-2로 표현할 수 있습니다. 

오늘날의 컴퓨터는 대부분 아래와 같은 방식으로 소수를 저장하는데 이와 같은 부동 소수점 저장 방식을 **IEEE 754** 라고 합니다.

<img src="https://github.com/user-attachments/assets/3aecf141-4cd0-4d28-8774-63ce2e779b00" width="500" height="200">

```
<IEEE 754 규칙>
1. 부호 비트는 양수면 0, 음수면 1이다.
2. 지수 부분은 bias가 더해지는데 bias = 2^(k-1)-1 (k는 지수의 비트수)로 지수가 8비트면 127, 11비트면 1023이다.
3. 가수 부분은 1.xxxxx 으로 만든 후 소수 부분인 xxxxx 부분만 저장한다.
```

예를 들어 2진수 1101011.1010101은 어떻게 저장될까요?

우선 가수 부분을 1.xxxxx과 같은 형태로 만들어줍니다. 

값을 1.1010111010101 x 2^6 로 바꾸어주고 값이 양수이니 부호비트 = 0, 지수는 6+127=133, 가수는 1010111010101이 됩니다.

<img src="https://github.com/user-attachments/assets/eeb8c381-d467-4ee8-ba04-f609190853a7" width="500" height="150">

이전에 언급했던 것처럼 여기서 유의할 점은 10진수 소수를 2진수로 표현할 때, **10진수와 2진수 사이에 오차가 존재할 수 있다는 것입니다.**

예를 들어 1/3을 m x 3^n 꼴로 표현하면 1 x 3^-1로 쉽게 표현할 수 있지만 m x 10^n 꼴로 표현하려면 0.33333.. 처럼 무한소수로 표현할 수 밖에 없습니다.

마찬가지로 10진수 소수인 0.1을 1.m x 2^n 꼴로 표현하는 것이 딱 맞아 떨어지지 않을 수도 있다는 것입니다. 

**이를 표현하기 위해서는 무한소수가 필요한데 컴퓨터는 무한소수를 저장할 수 없기 때문에 일부를 생략해서 저장하여 오차가 발생하게 됩니다.**

## 데이터 - 0과 1로 문자 표현하기

컴퓨터가 이해할 수 있는 문자들의 집합을 **문자 집합(Character Set)** 이라고 합니다. 

문자 집합에 속한 문자를 컴퓨터가 이해하는 0과 1로 변환하는 과정을 **문자 인코딩(Character Encoding)** 이라고 합니다.

반대로 0과 1을 문자로 변환하는 과정을 **문자 디코딩(Character Decoding)** 이라고 합니다. 

웹사이트를 보다보면 아래와 같은 문자 깨짐 현상을 종종 발견할 수 있습니다.

이는 웹사이트가 특정 인코딩 방법을 지원하지 않거나 인코딩된 문자를 디코딩하는 방법을 몰라 발생하는 문제입니다.

<img src="https://github.com/user-attachments/assets/361a60d7-8e98-4f11-b627-202d339c52ae" width="500" height="200">

가장 기본적인 문자 집합에는 **아스키(ASCII)** 가 있습니다. 하나의 아스키 문자를 표현하기 위해서는 8비트가 필요합니다. 

8비트 중 1비트는 **패리티 비트(Parity Bit)** 로 오류 검출을 위한 비트이기 때문에 실질적인 문자 표현에 사용되는 비트는 7비트입니다. 

따라서 2^7 = 128개의 문자를 표현할 수 있습니다. 

각 아스키 문자들은 0~127의 값 중 고유한 값에 대응되는데 이를 **아스키 코드**라고 부릅니다. 

예를 들어 ‘A’는 10진수 65 (2진수 1000001)로 인코딩되고, 'a'는 10진수 97 (2진수 1100001)로 인코딩됩니다.

<img src="https://github.com/user-attachments/assets/9a82f1de-ecc8-46e2-92d6-ecabd4e6cbd8" width="450" height="400">

위의 아스키 코드표를 보면 알 수 있듯, 아스키 문자에 대응된 고유한 수인 **아스키 코드를 2진수로 표현함으로써** 아스키 문자를 0과 1로 대응시킬 수 있습니다.

다만 아스키 코드는 한글을 표기할 수 없습니다. 그래서 등장한 한글 인코딩 방식 중 하나가 **EUC-KR** 입니다. 

EUC-KR은 KS X 1001, KS X 1003 이라는 문자 집합 기반의 인코딩 방식으로 아스키 문자에는 1바이트를, 하나의 한글 글자에는 2바이트를 필요로 합니다.

2바이트는 16비트로 네 자리 16진수로 표현할 수 있으므로 EUC-KR로 인코딩된 한글 글자 하나는 네 자리 16진수로 표현됩니다.

EUC-KR 방식을 사용하면 총 2350개 정도의 한글 문자를 표현할 수 있지만 아직도 많이 부족합니다. 

그래서 등장한 문자 집합이 **유니코드**입니다. 

유니코드는 한글을 포함해 여러 언어, 특수문자, 심지어 이모티콘까지 대부분의 문자를 지원하기 때문에 현대 가장 많이 사용되는 표준 문자 집합입니다.

아스키 코드나 EUC-KR처럼 유니코드 문자 집합에 포함된 문자에는 고유한 값이 부여되어 있습니다.

그러나 아스키 코드나 EUC-KR은 문자에 부여된 값을 그대로 인코딩 값으로 삼았지만 유니코드는 문자에 부여된 값을 다양한 방법으로 인코딩합니다.

대표적인 인코딩 방법에는 **UTF-8, UTF-16, UTF-32** 등이 있습니다.

UTF-8, UTF-16, UTF-32는 **가변 길이 인코딩 방식**으로 인코딩 결과의 길이가 일정하지 않을 수 있습니다. 

```python
a8 = '한'.encode('utf-8')
b8 = '글'.encode('utf-8')
print('utf-8 한:', a8.hex()) // utf-8 한 : ed959c
print('utf-8 글:', b8.hex()) // utf-8 글 : eab880

a16 = '한'.encode('utf-16') 
b16 = '글'.encode('utf-16') 
print('utf-16 한:', a16.hex()) // utf-16 한 : fffe5cd5
print('utf-16 글:', b16.hex()) // utf-16 글 : fffe00ae

a32 = '한'.encode('utf-32')
b32 = '글'.encode('utf-32')
print('utf-32 한:', a32.hex()) // utf-32 한 : fffe00005cd50000
print('utf-32 글:', b32.hex()) // utf-32 글 : fff3000000ae0000
```
<img src="https://github.com/user-attachments/assets/cdb72f82-a072-49a7-893b-c09dbefbaf23" width="450" height="200">

마지막으로 문자 뿐만 아니라 이진 데이터까지 변환할 수 있는 **base64 인코딩**에 대해 알아보겠습니다. 

base64는 문자 이외 이미지 같은 데이터들도 전부 아스키 문자 형태로 표현할 수 있습니다. 

base64는 사실 **64진법**을 사용하기 때문에 인코딩 값을 표현하기 위해 64개의 문자가 사용되며 64진수 하나를 표현하기 위해서는 6비트가 필요합니다.

따라서 변환할 데이터를 6비트씩 나누어 하나의 문자로 변환하게 됩니다. **기본적으로 4개씩(24비트씩) 한 번에 변환됩니다.**

<img src="https://github.com/user-attachments/assets/c4478a8e-ea42-4265-868b-6dcfd7985968" width="450" height="400">

예를 들어 ‘abc’가 base64 인코딩을 거치면 어떻게 될까요?

<img src="https://github.com/user-attachments/assets/0e4df2e1-5441-44a2-8a61-6ea3da889155" width="500" height="100">

a,b,c는 각각 아스키 코드 97,98,99로 인코딩 될 수 있는데 이 24비트 2진수를 6비트 단위로 끊어 변환하면 결국 ‘YWJj’ 가 됩니다.

<img src="https://github.com/user-attachments/assets/9a371cba-0ca9-4ac0-ac70-0154c1c4cb05" width="500" height="100">

만약 6비트로 나누어 떨어지지 않으면 어떻게 될까요? 나누어 떨어지지 않는 자리는 0으로 채워지는 패딩(Padding)이 되며 이는 ‘=’로 인코딩됩니다.

<img src="https://github.com/user-attachments/assets/df9e44f3-2d33-4a41-a0b3-e17dafeebece" width="500" height="100">

위의 그림에서 빈 부분을 '0'으로 채웠는데 왜 마지막 000000은 'A'가 아니라 '='로 변환될까요?

'ab'를 2진수로 표현하면 011000 / 010110 / 0010 과 같은데 이 때 마지막 블록(3번째 블록)이 4비트 뿐이라 2비트를 0으로 패딩합니다. 

base64 인코딩은 기본적으로 24비트씩, 즉 4개의 문자를 한 번에 변환해야 하기 때문에 마지막 블록은 '=' 패딩을 추가해줍니다.

또한 데이터를 6비트씩 묶어서 8비트의 문자로 변환하기 때문에 base64 인코딩은 **데이터 크기를 약 33% 가량 증가시킵니다.**

그럼에도 불구하고 base64 인코딩을 사용하는 이유는 무엇일까요?

아스키 코드 표를 보면 탭, 스페이스, 이스케이프 같은 제어 문자가 다수 포함되어 있습니다.

이런 제어 문자는 시스템마다 동작이 달라질 수 있기 때문에 아스키 문자를 사용하게 되면 시스템 간 데이터 전달에 문제가 생길 수 있습니다.

반면에 base64는 안전한 64개의 문자를 사용하기에 시스템 간 데이터 전송의 안정성과 호환성이 보장되어 많이 사용하는 것입니다.

## 명령어

<img src="https://github.com/user-attachments/assets/93539326-30cc-47cb-9522-f114ab90ecbd" width="500" height="200">

명령어는 수행할 동작과 수행할 대상으로 이루어져 있는데, 수행할 동작은 **연산 코드(Opcode)** 라고 부르고 수행할 대상은 **오퍼랜드(Operand)** 라고 부릅니다. 

보통 operand에는 데이터가 직접 저장되어 있기 보다는 데이터가 저장된 레지스터의 이름이나 메모리의 주소 값이 명시되는 경우가 많습니다. 

만약 메모리의 주소가 명시되어 있다면 명령어 실행을 위해 메모리를 접근해야 합니다.

<img src="https://github.com/user-attachments/assets/ebfc68d4-9d8b-48d1-bfb9-51b6143ead01" width="400" height="200">

데이터와 명령어를 CPU가 이해할 수 있도록 0과 1로 표현된 정보를 있는 그대로 표현한 언어를 **기계어(Machine Code)** 라고 합니다. 

그러나 이 기계어만 보고선 사람은 이것이 어떤 프로그램인지, PC가 어떻게 동작하는지 이해하기 어렵습니다. 

그래서 등장한 언어가 **어셈블리어(Assembly Language)입니다.** 어셈블리어를 보면 명령어의 종류와 동작을 파악할 수 있습니다.

어떤 언어로 어떤 프로그램을 만들든 컴퓨터는 내부에서 0과 1로 이루어진 기계어로 변환하여 프로그램을 실행합니다. 

이 때 연산 코드나 레지스터의 이름, 명령어의 생김새는 CPU마다 다를 수 있다는 점을 기억해야 합니다.

```java
int square(int num){
    return num * num;
}
```
위와 같은 간단한 소스 코드를 어셈블리어로 변환하면 다음과 같습니다.

<img src="https://github.com/user-attachments/assets/d44f6114-8123-4285-a44c-0ca9e91b0828" width="400" height="200">

<img src="https://github.com/user-attachments/assets/a508816a-6444-4fd6-8ffe-c37179406c2c" width="400" height="200">

앞서 CPU의 종류마다 이해하고 처리할 수 있는 명령어의 종류가 다르다고 했습니다. 

같은 제곱수를 반환하는 함수더라도 CISC 기반의 CPU, RISC 기반의 CPU가 소스코드를 어셈블리어로 변환한 형태가 달라집니다. 

사실 어셈블리어 자체를 아는 것보다 중요한 점은 **특정 CPU에 의존적인 프로그램 및 코드를 개발해선 안 된다는 것입니다.**

프로그램을 실행한다는 것은, CPU가 메모리 속 명령어를 인출하고 실행하기를 반복하는 것입니다. 

이 때 CPU가 명령어를 처리하는 과정에는 정형화된 흐름이 있습니다.

즉, CPU가 명령어를 처리하는 과정에서 명령어들은 일정한 주기를 반복하며 실행되는데 이 주기를 **명령어 사이클(Instruction Cycle)** 이라고 합니다.

CPU가 명령어를 실행하기 위해서는 먼저 명령어를 메모리에서 가져와야 하는데 이를 **인출 사이클(Fetch Cycle)** 이라고 합니다.

그 다음 인출한 명령어를 실행하는 단계를 **실행 사이클(Execution Cycle)** 이라고 합니다.

기본적으로는 인출 사이클과 실행 사이클이 반복됩니다.

<img src="https://github.com/user-attachments/assets/4d1ed9aa-17ec-4075-9559-14b0b09a33c8" width="250" height="250">

하지만 모든 명령어가 이렇게 간단하게 실행되지는 않습니다. 

Operand 필드에 메모리 주소가 명시되어 있다면 명령어를 인출하고 실행하기 전에 메모리에 접근해야 합니다.

이렇게 명령어 실행 전, 메모리에 접근하는 단계를 **간접 사이클(Indirect Cycle)** 이라고 합니다.

<img src="https://github.com/user-attachments/assets/9b59021f-b310-4f9f-b802-9b4936ceb78a" width="350" height="300">

추가로 인터럽트를 처리하는 **인터럽트 사이클(Interrupt Cycle)** 도 존재합니다.
