# 목차
- [물리 주소와 논리 주소](#물리-주소와-논리-주소)
- [스와핑과 연속 메모리 할당](#스와핑과-연속-메모리-할당)
  - [스와핑](#스와핑)
  - [연속 메모리 할당과 외부 단편화](#연속-메모리-할당과-외부-단편화)
- [페이징을 통한 가상 메모리 관리](#페이징을-통한-가상-메모리-관리)
  - [페이징](#페이징)
  - [세그멘테이션](#세그멘테이션)
  - [페이지 테이블](#페이지-테이블)
  - [페이징 주소 체계](#페이징-주소-체계)
- [페이지 교체 알고리즘](#페이지-교체-알고리즘)

CPU와 프로세스들이 메모리 몇 번지에 무엇이 저장되어 있는지 모두 알고 있지는 않습니다. 메모리 정보는 시시때때로 변경되기 때문입니다. 그렇다면 CPU는 어떻게 메모리에 적재된 프로세스의 주소를 인식하고 관리할까요?

## 물리 주소와 논리 주소

CPU와 프로세스는 메모리의 하드웨어 상 실제 주소인 **물리 주소(Physical Address)**가 아니라 **논리 주소(Logical Address)**를 이용합니다. 논리 주소는 프로세스마다 부여되는 0번지 부터 시작하는 주소 체계입니다. 메모리에 적재되어 있는 모든 프로세스마다 각자의 논리 주소를 가지고 있습니다. 따라서 중복되는 논리 주소의 번지 수는 얼마든지 존재할 수 있습니다.

그렇지만 실제로 데이터가 저장되어 있는 곳은 메모리 이기 때문에 **논리 주소와 물리 주소간의 변환**이 반드시 이루어져야 합니다. 이 변환을 담당하는 하드웨어가 메모리 관리 장치인 **MMU(Memory Management Unit)** 입니다.

<img src="https://github.com/user-attachments/assets/5ac989db-3deb-4c77-a5e4-679fde0e372a" width="400"/>

## 스와핑과 연속 메모리 할당

### 스와핑

메모리에 적재되어 있지만 실제로 실행되고 있지 않은 프로세스들도 있습니다. 입출력 I/O에 의해 대기 상태가 되었거나 오랫동안 사용 되지 않은 프로세스들인데요, 이러한 프로세스들을 보조기억장치의 일부 공간인 **스왑 영역(Swap Space)**로 보내고 메모리 여유 공간을 확보하는 메모리 관리 방식을 **스와핑(Swapping)** 이라고 합니다.

실행되지 않는 프로세스가 메모리 → 스왑 영역으로 옮겨지는 것을 **스왑 아웃(Swap-Out)**, 반대 과정을 **스왑 인(Swap-In)**이라고 하는데 프로세스가 스왑 아웃 되었다가 다시 스왑 인이 된다고 하더라도 다른 주소에 적재될 수 있습니다.

### 연속 메모리 할당과 외부 단편화

그림과 같이 메모리 내에 프로세스에 연속적인 메모리 공간을 할당하는 방식을 연속 메모리 할당이라고 합니다. 자연스러워 보이지만 사실 메모리 효율 측면에선 좋지 않은 방법입니다. 

<img src="https://github.com/user-attachments/assets/0ce87a6c-8bb5-4258-89f2-e4e380717edb" height="300" width="300"/>

프로세스를 연속적으로 적재한다면 아래 그림과 같이 메모리에 빈 공간이 총 50MB가 있을 때도 50MB의 프로세스를 적재하는 것은 불가능합니다.

<img src="https://github.com/user-attachments/assets/e06f3527-30e9-473d-860d-a3823347db48" height="300" width="300"/>

이렇게 메모리 사이의 빈 공간의 총합보다 크기가 작은 프로세스를 적재하지 못하는 현상을 **외부 단편화(External Fragmentation)** 이라고 합니다.

## 페이징을 통한 가상 메모리 관리

기존 스와핑과 연속 메모리 할당은 외부 단편화 문제와 물리 메모리보다 크기가 큰 프로세스는 실행할 수 없다는 2가지 문제점이 있습니다. 이 문제를 해결하는 기술이 바로 **가상 메모리(Virtual Memory)** 입니다.

가상 메모리 관리 기법에는 **페이징**과 **세그멘테이션**이 있습니다. 둘 중 범용적으로 사용되는 기법은 페이징입니다.

### 페이징

**페이징(Paging)**은 프로세스의 논리 주소 공간을 페이지라는 일정 단위로 나누고 물리 주소 공간을 페이지와 동일한 크기 단위인 프레임으로 나눈 뒤 페이지를 프레임에 할당하는 가상 메모리 관리 기법입니다.

<img src="https://github.com/user-attachments/assets/744a800b-655b-4e76-bcfc-f69239be5f44" height="400" width="500"/>

위와 같이 메모리를 할당하면 외부 단편화 문제가 발생하지 않을 뿐더러 프로세스 단위가 아니라 페이지 단위로 스왑 아웃/스왑 인이 이루어 진다면 프로세스의 일부만 메모리에 적재되는 것이 가능해 물리 메모리보다 큰 크기의 프로세스 실행도 가능해집니다.

### 세그멘테이션

**세그멘테이션(Segmentation)**은 페이징과 비슷하지만 일정한 크기의 페이지 단위가 아닌 가변적인 크기인 세그먼트 단위로 분할하는 방식이라 사실 세그먼트의 크기가 일정하지 않아 외부 단편화 문제가 발생할 수도 있습니다.

### 페이지 테이블

우리는 페이징이라는 가상 메모리 관리 기법을 통해 앞서 문제를 해결했습니다. 그러나 프로세스가 연속으로 적재되는 것이 아니기 때문에 CPU 입장에서는 다음으로 실행할 페이지의 위치를 찾기 어렵습니다. 이 문제를 해결하기 위해 페이지 번호와 프레임 번호의 대응 정보를 저장하고 있는 **페이지 테이블(Page Table)**을 활용합니다.

<img src="https://github.com/user-attachments/assets/f763459b-b362-45d3-9327-5257a1bc1267" height="400" width="600"/>

페이지 테이블에는 페이지 번호와 프레임 번호 이외에도 다양한 정보가 저장되어 있습니다. 페이지 테이블을 구성하는 하나의 행을 **페이지 테이블 엔트리(PTE)**라고 합니다.

<img src="https://github.com/user-attachments/assets/fe0c7692-bd44-4eba-ac95-47f65aa38d67" height="220" width="600"/>

**유효 비트(Valid Bit)**는 해당 페이지가 메모리에 적재되어 있는지를 표기합니다. 1이면 메모리에, 0이면 보조기억장치에 저장되어 있습니다. CPU는 보조기억장치에 직접 접근할 수 없기 때문에 보조기억장치에 저장된 페이지를 메모리로 적재한 뒤에 접근합니다. 이를 **페이지 폴트(Page Fault)**라고 합니다. 

CPU의 페이지 폴트 처리 과정은 다음과 같습니다.

1. 기존 작업 내역을 백업합니다.
2. 페이지 폴트 처리 루틴을 실행합니다. 페이지를 메모리에 적재하고 유효 비트를 1로 만들어 줍니다.
3. 이후 메모리에 적재된 페이지를 실행합니다.

앞서 컴퓨터구조 챕터에서 ‘폴트’에 대해 배웠습니다. ‘폴트’는 예외가 발생한 명령어부터 실행을 재개하는 예외입니다. 따라서 페이지 폴트도 메모리를 적재한 후 해당 페이지부터 다시 실행하는 것입니다.

**보호 비트(Protection Bit)**는 read, write, execute에 대한 페이지 권한을 설정합니다.

**참조 비트(Reference Bit)**는 해당 페이지에 접근한 적이 있는지의 여부를 나타냅니다. 참조한 적이 있다면 1, 없으면 0 입니다.

**수정 비트(Modified Bit)**는 해당 페이지에 데이터를 쓴 적이 있는지의 여부를 알려주는 비트로 **더티 비트(Dirty Bit)**라고도 합니다. 수정 비트가 1일 경우, 페이지의 내용이 변경 되었기 때문에 추후에 보조기억장치에도 변경 사항을 반영해주어야 합니다.

그리고 아래 그림과 같이 논리 주소 공간이 페이지 단위로 딱 맞게 잘리지 않을 수도 있습니다. 이렇게 발생하는 메모리 낭비를 **내부 단편화(Internal Framentation)**이라고 합니다.

<img src="https://github.com/user-attachments/assets/afbd3cf9-0ff0-462a-870f-ec6a85888985" height="300" width="400"/>

페이징 기법을 사용하면 외부 단편화를 해결할 수 있지만 내부 단편화 문제가 발생하고, 세그먼트 기법을 사용하면 반대로 내부 단편화를 해결할 수 있지만 외부 단편화 문제가 발생합니다.

그런데 페이지 테이블은 어디에 저장될까요? 사실 메모리에 적재됩니다. 따라서 CPU는 페이지 테이블의 위치를 알아야 하는데 특정 프로세스의 페이지 테이블을 가리키는 레지스터가 **페이지 테이블 베이스 레지스터(PTBR)** 입니다. PTBR는 프로세스마다 가지고 있기 때문에 PCB에 저장됩니다.

페이지 테이블은 프로세스마다 존재합니다. 따라서 프로세스의 개수가 많아지면 그에 따라 페이지 테이블의 개수도 많아집니다. 그런데 이 **모든 페이지 테이블을 메모리에 두는 것은** 1️⃣ 메모리 접근 횟수가 많아지고 2️⃣ 메모리 용량을 많이 차지하기 때문에 **비효율적입니다**. 

1️⃣ 메모리 접근 횟수

CPU는 특정 페이지에 접근하기 위해 페이지 테이블 접근을 위해 1번, 실제 프레임에 접근하기 위해 1번, 총 2번 메모리 접근을 합니다. 이런 메모리 접근 횟수 증가를 막기 위해 **TLB(Translation Look-aside Buffer)**라는 페이지 테이블에 대한 캐시가 등장합니다.

만약 CPU가 접근하려는 페이지가 TLB에 있다면 (TLB Hit), 한 번만 메모리에 접근하면 됩니다. 반면에 TLB Miss인 경우에는 어쩔 수 없이 페이지 테이블에 접근 해야겠죠. 그래서 TLB Hit율을 높이는 것이 중요합니다.

<img src="https://github.com/user-attachments/assets/4999cf15-dac8-4349-9c4d-fd7acd9fcc45" height="300" width="550"/>

2️⃣ 메모리 용량

페이지 테이블의 크기는 생각보다 작지 않기 때문에 모든 PTE를 메모리에 두는 것은 낭비입니다. 그래서 우리는 **계층적 페이징(Hierarchical Paging)** 방식을 사용합니다. **다단계 페이지 테이블(MultiLevel Page Table)**이라고도 합니다.

페이지 테이블을 여러 개로 나누어 각 조각을 가리키는 Outer 페이지 테이블을 두고 Outer 페이지 테이블만 메모리에 유지하면 페이지 테이블 조각들이 보조기억장치에 있더라도 언제든지 접근할 수 있습니다.

<img src="https://github.com/user-attachments/assets/042f8b5d-cb12-43c3-a80e-e0b148e8f21e" height="300" width="400"/>

예를 들어봅시다.

32bit 운영체제에서 논리 주소 공간의 크기는 2^32bit입니다. 보통 페이지 단위로 4KB를 사용한다고 하니 프로세스 1개에 대해서 2^20개의 페이지가 존재합니다. 또한 PTE는 보통 4byte이기 때문에 페이지 테이블의 크기는 2^20 x 4byte = 4MB입니다. 따라서 프로세스 100개가 실행된다고 하면 페이지 테이블의 크기만 400MB가 되며 64bit 운영체제에선 이보다 크기가 훨씬 커집니다. 따라서 테이블의 일부만 적재하려고 하는 것입니다.

그런데 이 계층적 페이징 방식에서 계층을 나누면 나눌수록 보조기억장치를 자주 참조해야 하는 단점이 있습니다. 그래서 계층적 페이징 외에 해시 페이지 테이블, 역페이지 테이블과 같은 방법들도 있습니다.

(참고 : https://itstory1592.tistory.com/103)

### 페이징 주소 체계

논리 주소는 <페이지 번호, 변위>의 형태로 이루어져 있습니다. 여기서 변위란, 해당 페이지의 시작 주소로부터 얼만큼 떨어져 있는지를 나타내는 정보입니다. 이 논리 주소가 물리 주소인 <프레임 번호, 변위>로 변환됩니다.

<img src="https://github.com/user-attachments/assets/fb3b13cd-74e0-4e3b-846c-33baeec8e64b" height="300" width="550"/>

## 페이지 교체 알고리즘

앞서 논리 주소 공간의 모든 페이지가 메모리에 적재될 필요가 없다고 했습니다. 메모리에 필요한 페이지를 적재하다보면 메모리에 페이지가 가득 찬 상황이 발생하고 이 때 페이지를 적절하게 선택해서 스왑 아웃 시켜 메모리 공간을 확보해야 합니다. 어떤 페이지를 스왑 아웃 시킬 지를 결정하는 방법이 **페이지 교체 알고리즘** 입니다. 

캐시를 사용할 때 캐시 히트율을 높이기 위해 적절한 데이터를 선택하는 것처럼 페이지 폴트의 발생 빈도를 줄이기 위해 어떤 페이지 교체 알고리즘을 선택하는 지가 성능에 큰 영향을 끼칩니다. 

1️⃣ FIFO (First In First Out)

FIFO는 메모리에 가장 먼저 적재된 페이지부터 교체합니다. 간단하지만 자주 참조되는 페이지가 스왑 아웃될 수 있어 성능이 좋지는 않습니다.

2️⃣ 최적 페이지 교체 알고리즘

앞으로의 사용 빈도가 가장 낮은 페이지를 예측해서 교체하는데 ‘예측’하는 것 자체가 어려워 실제 구현이 어렵습니다.

3️⃣ LRU (Least Recently Used)

LRU는 가장 적게 사용한 페이지를 교체하는 알고리즘으로 가장 보편적으로 사용됩니다.
